{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Boosting is an ensemble learning technique in machine learning where multiple weak learners (models that perform slightly better than random guessing) are combined sequentially to create a strong learner that has high predictive accuracy.\n",
        "\n",
        "Weak learners are usually shallow decision trees.\n",
        "\n",
        "Unlike Bagging (where models are independent), Boosting trains models one after another, with each new model focusing on the mistakes of the previous ones.\n",
        "\n",
        "2. Key Idea Behind Boosting\n",
        "\n",
        "“Focus on the errors of previous models and improve step by step.”\n",
        "\n",
        "Each model tries to correct the errors made by the previous model.\n",
        "\n",
        "The predictions of all models are combined (weighted sum or vote) to produce the final prediction.\n",
        "\n",
        "This way, even if individual models are weak, together they form a strong, accurate model.\n",
        "\n",
        "3. How Boosting Improves Weak Learners\n",
        "\n",
        "Sequential Training:\n",
        "\n",
        "First model fits the data.\n",
        "\n",
        "Identify the samples it predicts incorrectly.\n",
        "\n",
        "Give these misclassified samples higher weights for the next model.\n",
        "\n",
        "Weighted Contribution:\n",
        "\n",
        "Each weak learner is assigned a weight based on its accuracy.\n",
        "\n",
        "Better-performing models contribute more to the final prediction.\n",
        "\n",
        "Error Reduction:\n",
        "\n",
        "By focusing on hard-to-predict examples, Boosting reduces bias (underfitting) and improves accuracy.\n"
      ],
      "metadata": {
        "id": "KMBjkzhVBpsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Answer:\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Training approach:\n",
        "\n",
        "Start with equal weights for all training samples.\n",
        "\n",
        "Train a weak learner (usually a shallow decision tree).\n",
        "\n",
        "Increase weights of misclassified samples, so the next learner focuses more on these “hard” cases.\n",
        "\n",
        "Each learner is assigned a weight based on its accuracy.\n",
        "\n",
        "Final model = weighted vote (classification) or weighted sum (regression).\n",
        "\n",
        "Key Idea: Adapt sample weights → misclassified points get higher importance.\n",
        "\n",
        "2. Gradient Boosting\n",
        "\n",
        "Training approach:\n",
        "\n",
        "Start with a simple model (e.g., a constant prediction like mean for regression).\n",
        "\n",
        "Compute the residual errors (difference between actual and predicted values).\n",
        "\n",
        "Train the next weak learner to predict these residuals (gradients of loss function).\n",
        "\n",
        "Update the model by adding the new learner’s predictions (with a learning rate).\n",
        "\n",
        "Repeat until convergence.\n",
        "\n",
        "Key Idea: Optimize the loss function by fitting new learners to the gradients of errors, not by reweighting samples."
      ],
      "metadata": {
        "id": "wsnAQKn_CGRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** How does regularization help in XGBoost?\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. What is XGBoost?\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful implementation of gradient boosting that is optimized for speed, scalability, and accuracy.\n",
        "\n",
        "A key feature of XGBoost compared to standard Gradient Boosting is its built-in regularization.\n",
        "\n",
        "2. Why Regularization Matters\n",
        "\n",
        "Boosting models (like Gradient Boosting) are prone to overfitting, especially when:\n",
        "\n",
        "Trees are too deep\n",
        "\n",
        "Too many boosting rounds are used\n",
        "\n",
        "Data has noise\n",
        "\n",
        "Regularization helps by penalizing model complexity, making the model more generalizable.\n",
        "\n",
        "3. How XGBoost Uses Regularization\n",
        "\n",
        "XGBoost adds a regularization term to its objective function:\n",
        "\n",
        " → L2 regularization on leaf weights\n",
        "\n",
        "Prevents overly large predictions from leaves.\n",
        "\n",
        "Shrinks weights, reducing overfitting.\n",
        "\n",
        "Additionally:\n",
        "\n",
        "XGBoost also supports L1 regularization (α) on leaf weights → encourages sparsity (some weights become 0).\n",
        "\n",
        "4. Regularization Parameters in XGBoost\n",
        "\n",
        "lambda (reg_lambda): L2 regularization term on weights\n",
        "\n",
        "alpha (reg_alpha): L1 regularization term on weights\n",
        "\n",
        "gamma (min_split_loss): Minimum loss reduction required to split a node → prevents unnecessary splits\n",
        "\n",
        "5. Intuition\n",
        "\n",
        "Without regularization → trees might grow too complex, fitting noise.\n",
        "\n",
        "With regularization → trees are simpler, more robust, and less likely to overfit."
      ],
      "metadata": {
        "id": "whdgPituCcAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer:\n",
        "1. Traditional Problem with Categorical Data\n",
        "\n",
        "Most machine learning models (including standard Gradient Boosting) cannot handle categorical features directly.\n",
        "\n",
        "They require preprocessing, like:\n",
        "\n",
        "One-hot encoding → can explode feature space for high-cardinality categories\n",
        "\n",
        "Label encoding → can introduce artificial ordinal relationships\n",
        "\n",
        "Both approaches have drawbacks: inefficiency, high memory usage, or bias.\n",
        "\n",
        "2. How CatBoost Handles Categorical Data\n",
        "\n",
        "CatBoost introduces a technique called “Ordered Target Encoding” (or mean encoding with ordering) that allows it to handle categorical features natively.\n",
        "\n",
        "Key points:\n",
        "\n",
        "Transforms categories into numbers internally using the target variable.\n",
        "\n",
        "Avoids target leakage by using an ordered boosting process, where the target statistic for a row is computed only from previous rows.\n",
        "\n",
        "Handles high-cardinality features efficiently without exploding the feature space.\n",
        "\n",
        "Automatically deals with missing values in categorical features.\n",
        "\n",
        "3. Advantages\n",
        "\n",
        "No need for extensive preprocessing (less memory, less computation).\n",
        "\n",
        "Reduces overfitting compared to standard target encoding.\n",
        "\n",
        "Works well with datasets that have many categorical features.\n",
        "\n",
        "Often performs better out-of-the-box compared to XGBoost or LightGBM on categorical-heavy datasets.\n",
        "\n",
        "4. Intuition\n",
        "\n",
        "Suppose you have a feature City and the target is Loan Default.\n",
        "\n",
        "CatBoost will encode “City” based on the historical average default rate, but in an ordered fashion so that it doesn’t leak future information during training.\n",
        "\n",
        "This avoids introducing bias or overfitting while still capturing predictive information.\n"
      ],
      "metadata": {
        "id": "Etb40KqTCyUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Answer:\n",
        "1. Credit Scoring and Loan Default Prediction\n",
        "\n",
        "Why Boosting?\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM, CatBoost) is very good at detecting rare events like defaults.\n",
        "\n",
        "It focuses on misclassified examples, so it can capture subtle patterns in borrowers with higher default risk.\n",
        "\n",
        "Example: Banks predicting whether a customer will default on a loan using demographic and transaction history.\n",
        "\n",
        "2. Fraud Detection\n",
        "\n",
        "Why Boosting?\n",
        "\n",
        "Fraudulent transactions are rare and complex patterns.\n",
        "\n",
        "Boosting’s sequential training helps focus on these rare, hard-to-detect cases.\n",
        "\n",
        "Example: Credit card fraud detection, insurance claim fraud.\n",
        "\n",
        "3. Marketing and Customer Churn Prediction\n",
        "\n",
        "Why Boosting?\n",
        "\n",
        "Boosting can model complex non-linear relationships in customer behavior.\n",
        "\n",
        "It handles imbalanced datasets well (e.g., only a small fraction of customers churn).\n",
        "\n",
        "Example: Telecom companies predicting customer churn to offer retention incentives.\n",
        "\n",
        "4. Online Advertising and Click-Through Rate (CTR) Prediction\n",
        "\n",
        "Why Boosting?\n",
        "\n",
        "Boosting can capture subtle interactions between user features and ad features.\n",
        "\n",
        "Many advertising platforms use LightGBM or XGBoost to predict which users are likely to click an ad.\n",
        "\n",
        "5. Healthcare Risk Prediction\n",
        "\n",
        "Why Boosting?\n",
        "\n",
        "Patient risk datasets often have imbalanced outcomes (e.g., predicting rare diseases).\n",
        "\n",
        "Boosting can focus on misclassified high-risk cases and improve detection accuracy.\n",
        "\n",
        "Example: Predicting which patients are at risk of readmission or complications.\n",
        "\n",
        "6. Key Reasons Boosting is Preferred\n",
        "\n",
        "Handles imbalanced datasets better (rare events).\n",
        "\n",
        "Reduces bias by combining weak learners sequentially.\n",
        "\n",
        "Captures subtle patterns that bagging might miss.\n",
        "\n",
        "Often achieves higher predictive accuracy than bagging on complex datasets."
      ],
      "metadata": {
        "id": "Ys1VcSLoC-6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets:\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "**Question 6:** Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "Explanation\n",
        "\n",
        "Dataset: load_breast_cancer() provides features and labels for classification.\n",
        "\n",
        "Train-test split: 70% training, 30% testing, stratified to preserve class distribution.\n",
        "\n",
        "AdaBoost: Uses default Decision Stumps as weak learners.\n",
        "\n",
        "Evaluation: Accuracy is computed on the test set."
      ],
      "metadata": {
        "id": "imthnYIRDMaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIVMR18Bjsi",
        "outputId": "18916b4b-3c0d-4f07-cd1d-0fddd01fb927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.953\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train an AdaBoost Classifier\n",
        "ada = AdaBoostClassifier(\n",
        "    n_estimators=100,   # number of weak learners\n",
        "    learning_rate=1.0,  # step size\n",
        "    random_state=42\n",
        ")\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = ada.predict(X_test)\n",
        "\n",
        "# 5. Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Explanation\n",
        "\n",
        "Dataset: fetch_california_housing() provides numerical features for regression.\n",
        "\n",
        "Train-test split: 80% training, 20% testing.\n",
        "\n",
        "Gradient Boosting: Combines many shallow trees sequentially to reduce bias and improve predictions.\n",
        "\n",
        "Evaluation: r2_score measures how well the model predicts the variance in the target; 1.0 is perfect prediction."
      ],
      "metadata": {
        "id": "k-_BdNlUDsvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=100,      # number of boosting stages\n",
        "    learning_rate=0.1,     # step size\n",
        "    max_depth=3,           # max depth of individual trees\n",
        "    random_state=42\n",
        ")\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# 5. Evaluate performance using R-squared\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score of Gradient Boosting Regressor: {r2:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "defBrmYADj_p",
        "outputId": "627e7bba-6c13-47ee-cf2f-d821764b9c6e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score of Gradient Boosting Regressor: 0.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "Answer:\n",
        "\n",
        "Explanation\n",
        "\n",
        "Dataset: Breast Cancer dataset for binary classification.\n",
        "\n",
        "XGBoost: Powerful gradient boosting implementation.\n",
        "\n",
        "Hyperparameter tuning: learning_rate controls how much each tree contributes; GridSearchCV finds the best value.\n",
        "\n",
        "Evaluation: Accuracy is computed on the test set using the tuned model.\n"
      ],
      "metadata": {
        "id": "x9VGiV41EEuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Define XGBoost classifier\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 4. Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters and final accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Learning Rate:\", best_params['learning_rate'])\n",
        "print(f\"Accuracy on Test Set: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66qGQDVWEEUS",
        "outputId": "e1528e36-1a58-4554-9160-235d2d19981c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate: 0.3\n",
            "Accuracy on Test Set: 0.965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [19:13:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "Answer:\n",
        "\n",
        "Explanation\n",
        "\n",
        "CatBoostClassifier: Handles categorical and numerical data efficiently (here all features are numerical).\n",
        "\n",
        "Confusion Matrix: Shows the number of true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "Seaborn heatmap: Provides a clear visual representation of model performance."
      ],
      "metadata": {
        "id": "ecbxixGBEli8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o9oQ-CxFdcL",
        "outputId": "73732398-a4a6-4488-bd54-0af1ab223a73"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    verbose=0,         # Suppress training output\n",
        "    random_seed=42     # Correct parameter for reproducibility\n",
        ")\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# 5. Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 6. Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "jBt6Y6SfEAuB",
        "outputId": "65969f2e-f029-4c06-ef79-0b7011271259"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAHWCAYAAAAmWbC9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM0NJREFUeJzt3XucjVX///H3njGzzXkMc0QzgwiJCNXIocgpYW45dFdDqaQipOi+y6FwR1JRoRQpEgq3Ds6losixhJwPMU4xgzFjzKzfH772zzYzzGLGnm6v5+OxH5l1rb2uz3XZO++99rqucRhjjAAAACx4eboAAADw90OAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAQJGxZcsW3X333QoJCZHD4dCsWbMKdPydO3fK4XBo4sSJBTru31nDhg3VsGFDT5eBQlYUXvtxcXHq3LmzW1tu7/mJEyfK4XBo586dHqkT+UeAgJtt27bp8ccfV7ly5VS8eHEFBwcrISFBb775pk6dOlWo+05KStKvv/6qIUOGaPLkybrlllsKdX9XU+fOneVwOBQcHJzredyyZYscDoccDodee+016/H37dungQMHau3atQVQ7dWTlZWlDz/8UA0bNlRYWJicTqfi4uLUpUsX/fLLL9bj/f777xo4cGCu//g0bNjQdY4dDod8fX0VHx+vxx57THv27CmAo7kyy5Yt08CBA3Xs2DGr53377bdKTExUVFSUfH19FRERoVatWunzzz8vnEIL0P/ye/6aYID/M3fuXOPn52dCQ0NNjx49zPjx482YMWNMx44djY+Pj3n00UcLbd9paWlGkvnXv/5VaPvIzs42p06dMmfOnCm0feQlKSnJFCtWzHh7e5tp06bl2D5gwABTvHhxI8mMGDHCevyVK1caSebDDz+0el5GRobJyMiw3l9BSEtLM82aNTOSTP369c2IESPMhAkTzIsvvmgqVapkHA6H2bNnj9WY06dPN5LMkiVLcmxr0KCBKVOmjJk8ebKZPHmymTBhgunTp48JCAgw1113nTl58mQBHdnlGTFihJFkduzYke/nvPTSS0aSuf76681LL71kJkyYYIYPH24aNmxoJJlPPvnEGGPMjh07Luv1UZDS09PN6dOnXT/n9Z4/c+aMOXXqlMnOzr7aJcJSMU8FFxQtO3bsUMeOHRUbG6vFixcrOjrate3JJ5/U1q1b9eWXXxba/g8dOiRJCg0NLbR9OBwOFS9evNDGvxSn06mEhARNnTpV7du3d9s2ZcoUtWzZUjNnzrwqtaSlpcnf31++vr5XZX+56du3r7755huNGjVKzzzzjNu2AQMGaNSoUQW+z5CQED3wwANubfHx8Xrqqaf0448/qkmTJgW+z8IyY8YMDR48WO3atdOUKVPk4+Pj2ta3b1/NmzdPmZmZHqzQndPpdPs5r/e8t7e3vL29C2y/J0+eVEBAQIGNh/N4OsGgaOjWrZuRZH788cd89c/MzDSDBw825cqVM76+viY2Ntb079/fpKenu/WLjY01LVu2NN9//72pXbu2cTqdJj4+3kyaNMnVZ8CAAUaS2yM2NtYYc/aT+7k/n+/cc843f/58k5CQYEJCQkxAQICpWLGi6d+/v2t7Xp/CFi1aZOrVq2f8/f1NSEiIuffee83vv/+e6/62bNlikpKSTEhIiAkODjadO3fO1yfXpKQkExAQYCZOnGicTqc5evSoa9uKFSuMJDNz5swcMxBHjhwxffr0MTfeeKMJCAgwQUFBplmzZmbt2rWuPkuWLMlx/s4/zgYNGpiqVauaX375xdxxxx3Gz8/P9OzZ07WtQYMGrrEeeugh43Q6cxz/3XffbUJDQ82ff/55yWPNjz179phixYqZJk2a5Kv/zp07zRNPPGEqVqxoihcvbsLCwky7du3cPq1/+OGHuZ6Hc7MR587DhWbMmGEkmcWLF7u1r1692jRr1swEBQWZgIAAc+edd5rly5fneP62bdtMu3btTIkSJYyfn5+pW7eumTt3bo5+b731lqlSpYprlq9WrVquGYLc3gO6xGzEDTfcYMLCwkxqauolz19ur/1169aZpKQkEx8fb5xOp4mMjDRdunQxhw8fdntuamqq6dmzp4mNjTW+vr4mPDzcNG7c2KxatcrV548//jCJiYkmMjLSOJ1OU7p0adOhQwdz7NgxV5/Y2FiTlJSU5/Gee5+f+3u88Ni/+uor1/s0MDDQtGjRwvz2229ufc69z7Zu3WqaN29uAgMDTevWrS95fnB5mIGAJOm///2vypUrp9tvvz1f/bt27apJkyapXbt26tOnj37++WcNGzZMGzdu1BdffOHWd+vWrWrXrp0eeeQRJSUl6YMPPlDnzp1Vq1YtVa1aVYmJiQoNDVWvXr3UqVMntWjRQoGBgVb1b9iwQffcc49uuukmDR48WE6nU1u3btWPP/540ectXLhQzZs3V7ly5TRw4ECdOnVKo0ePVkJCglavXq24uDi3/u3bt1d8fLyGDRum1atX6/3331dERIReffXVfNWZmJiobt266fPPP9fDDz8s6ezsww033KCaNWvm6L99+3bNmjVL9913n+Lj43XgwAGNGzdODRo00O+//66YmBhVrlxZgwcP1ksvvaTHHntMd9xxhyS5/V0eOXJEzZs3V8eOHfXAAw8oMjIy1/refPNNLV68WElJSVq+fLm8vb01btw4zZ8/X5MnT1ZMTEy+jvNSvv76a505c0YPPvhgvvqvXLlSy5YtU8eOHVWmTBnt3LlT7777rho2bKjff/9d/v7+ql+/vnr06KG33npLL7zwgipXrixJrv9KZ9dcHD58WJKUmZmpjRs3asCAAapQoYISEhJc/TZs2KA77rhDwcHBeu655+Tj46Nx48apYcOG+u6771S3bl1J0oEDB3T77bcrLS1NPXr0UMmSJTVp0iTde++9mjFjhtq2bStJeu+999SjRw+1a9dOPXv2VHp6utavX6+ff/5Z999/vxITE/XHH39o6tSpGjVqlEqVKiVJCg8Pz/V8bNmyRZs2bdLDDz+soKAgy7N/1oIFC7R9+3Z16dJFUVFR2rBhg8aPH68NGzbop59+ksPhkCR169ZNM2bM0FNPPaUqVaroyJEj+uGHH7Rx40bVrFlTp0+fVtOmTZWRkaGnn35aUVFR+vPPPzV37lwdO3ZMISEhOfZt+56fPHmykpKS1LRpU7366qtKS0vTu+++q3r16mnNmjVu79MzZ86oadOmqlevnl577TX5+/tf1vlBPng6wcDzUlJSjKR8J/W1a9caSaZr165u7c8++2yOT3KxsbFGklm6dKmr7eDBg8bpdJo+ffq42s59Qrrw+//8zkCMGjXKSDKHDh3Ks+7cPoXVqFHDREREmCNHjrja1q1bZ7y8vMxDDz2UY38PP/yw25ht27Y1JUuWzHOf5x9HQECAMcaYdu3ambvuussYY0xWVpaJiooygwYNyvUcpKenm6ysrBzH4XQ6zeDBg11tF1sD0aBBAyPJjB07Ntdt589AGGPMvHnzjCTzyiuvmO3bt5vAwEDTpk2bSx6jjV69ehlJZs2aNfnqn5aWlqNt+fLlRpL56KOPXG2XWgOhXD7lV65c2Wzfvt2tb5s2bYyvr6/Ztm2bq23fvn0mKCjI1K9f39X2zDPPGEnm+++/d7UdP37cxMfHm7i4ONffXevWrXOd/TifzRqI2bNnG0lm1KhRl+xrTO6v/dzO6dSpU3O8X0NCQsyTTz6Z59hr1qwxksz06dMvWsP5MxDn13The/7CGYjjx4+b0NDQHGuwkpOTTUhIiFt7UlKSkWT69et30VpQMLgKA0pNTZWkfH+S+eqrryRJvXv3dmvv06ePJOVYK1GlShXXp2Lp7KeqSpUqafv27Zdd84XOfY86e/ZsZWdn5+s5+/fv19q1a9W5c2eFhYW52m+66SY1adLEdZzn69atm9vPd9xxh44cOeI6h/lx//3369tvv1VycrIWL16s5ORk3X///bn2dTqd8vI6+zbNysrSkSNHFBgYqEqVKmn16tX53qfT6VSXLl3y1ffuu+/W448/rsGDBysxMVHFixfXuHHj8r2v/LB9zfn5+bn+nJmZqSNHjqhChQoKDQ21Og9xcXFasGCBFixYoK+//lpvvPGGUlJS1Lx5c9d38llZWZo/f77atGmjcuXKuZ4bHR2t+++/Xz/88IOr/q+++kp16tRRvXr1XP0CAwP12GOPaefOnfr9998lnX197t27VytXrsx3rRdje/5yc/45TU9P1+HDh3XrrbdKkts5DQ0N1c8//6x9+/blOs65GYZ58+YpLS3tsuvJy4IFC3Ts2DF16tRJhw8fdj28vb1Vt25dLVmyJMdznnjiiQKvAzkRIKDg4GBJ0vHjx/PVf9euXfLy8lKFChXc2qOiohQaGqpdu3a5tV933XU5xihRooSOHj16mRXn1KFDByUkJKhr166KjIxUx44d9dlnn100TJyrs1KlSjm2Va5cWYcPH9bJkyfd2i88lhIlSkiS1bG0aNFCQUFBmjZtmj755BPVrl07x7k8Jzs7W6NGjdL1118vp9OpUqVKKTw8XOvXr1dKSkq+91m6dGmrBZOvvfaawsLCtHbtWr311luKiIi45HMOHTqk5ORk1+PEiRN59rV9zZ06dUovvfSSypYt63Yejh07ZnUeAgIC1LhxYzVu3FjNmjVTz549NWfOHG3evFn/+c9/XMeRlpaW5+siOzvbddnnrl278ux3brskPf/88woMDFSdOnV0/fXX68knn7zk12sXY3v+cvPXX3+pZ8+eioyMlJ+fn8LDwxUfHy9Jbud0+PDh+u2331S2bFnVqVNHAwcOdAv/8fHx6t27t95//32VKlVKTZs21dtvv23193IxW7ZskSTdeeedCg8Pd3vMnz9fBw8edOtfrFgxlSlTpkD2jYsjQEDBwcGKiYnRb7/9ZvW8c9+RXkpeK6qNMZe9j6ysLLef/fz8tHTpUi1cuFAPPvig1q9frw4dOqhJkyY5+l6JKzmWc5xOpxITEzVp0iR98cUXec4+SNLQoUPVu3dv1a9fXx9//LHmzZunBQsWqGrVqvmeaZHcP23mx5o1a1z/Y/7111/z9ZzatWsrOjra9bjY/SxuuOEGq7GffvppDRkyRO3bt9dnn32m+fPna8GCBSpZsqTVechNrVq1FBISoqVLl17ROBdTuXJlbd68WZ9++qnq1aunmTNnql69ehowYMBljWd7/nLTvn17vffee641OfPnz9c333wjSW7ntH379tq+fbtGjx6tmJgYjRgxQlWrVtXXX3/t6jNy5EitX79eL7zwgk6dOqUePXqoatWq2rt372XXd865WiZPnuyaPTr/MXv2bLf+58/aoXCxiBKSpHvuuUfjx4/X8uXLddttt120b2xsrLKzs7Vlyxa3BWoHDhzQsWPHFBsbW2B1lShRItcb61w4yyFJXl5euuuuu3TXXXfp9ddf19ChQ/Wvf/1LS5YsUePGjXM9DknavHlzjm2bNm1SqVKlCu3yr/vvv18ffPCBvLy81LFjxzz7zZgxQ40aNdKECRPc2o8dO+ZaaCflP8zlx8mTJ9WlSxdVqVJFt99+u4YPH662bduqdu3aF33eJ5984naTrPOn/y/UvHlzeXt76+OPP87XQsoZM2YoKSlJI0eOdLWlp6fneG1c7nnIyspyzZiEh4fL398/z9eFl5eXypYtK+nsayivfue2nxMQEKAOHTqoQ4cOOn36tBITEzVkyBD1799fxYsXt6q9YsWKqlSpkmbPnq0333zTetHx0aNHtWjRIg0aNEgvvfSSq/3cp/0LRUdHq3v37urevbsOHjyomjVrasiQIWrevLmrT7Vq1VStWjX9+9//1rJly5SQkKCxY8fqlVdesartQuXLl5ckRURE5Po+hucQ0yBJeu655xQQEKCuXbvqwIEDObZv27ZNb775pqSzU/CS9MYbb7j1ef311yVJLVu2LLC6ypcvr5SUFK1fv97Vtn///hxXevz11185nlujRg1JUkZGRq5jR0dHq0aNGpo0aZLbP0S//fab5s+f7zrOwtCoUSO9/PLLGjNmjKKiovLs5+3tnWN2Y/r06frzzz/d2s4FHdu7GObm+eef1+7duzVp0iS9/vrriouLU1JSUp7n8ZyEhATX1wONGze+aIAoW7asHn30Uc2fP1+jR4/OsT07O1sjR450fYLN7TyMHj06x+zS5ZyHJUuW6MSJE6pevbprX3fffbdmz57tdkfLAwcOaMqUKapXr57rK4QWLVpoxYoVWr58uavfyZMnNX78eMXFxalKlSqSzl4Fcz5fX19VqVJFxhjXvRpsax80aJCOHDmirl276syZMzm2z58/X3Pnzs31uedm0i48pxe+p7OysnJ8FREREaGYmBjX6yE1NTXH/qtVqyYvL69Lvmbyo2nTpgoODtbQoUNzva/FubUruPqYgYCks/9QT5kyRR06dFDlypX10EMP6cYbb9Tp06e1bNkyTZ8+3XUf++rVqyspKUnjx4/XsWPH1KBBA61YsUKTJk1SmzZt1KhRowKrq2PHjnr++efVtm1b9ejRw3X5VsWKFd0Weg0ePFhLly5Vy5YtFRsbq4MHD+qdd95RmTJl3Ba4XWjEiBFq3ry5brvtNj3yyCOuyzhDQkI0cODAAjuOC3l5eenf//73Jfvdc889Gjx4sLp06aLbb79dv/76qz755JMc/ziXL19eoaGhGjt2rIKCghQQEKC6deu6vtPOr8WLF+udd97RgAEDXJeVnrvV9Isvvqjhw4dbjXcxI0eO1LZt29SjRw99/vnnuueee1SiRAnt3r1b06dP16ZNm1yzM/fcc48mT56skJAQValSRcuXL9fChQtVsmRJtzFr1Kghb29vvfrqq0pJSZHT6dSdd97pWsORkpKijz/+WNLZy/02b96sd999V35+furXr59rnFdeeUULFixQvXr11L17dxUrVkzjxo1TRkaG2zno16+fpk6dqubNm6tHjx4KCwvTpEmTtGPHDs2cOdM1lX733XcrKipKCQkJioyM1MaNGzVmzBi1bNnStRCyVq1akqR//etf6tixo3x8fNSqVas8Z8E6dOjgug30mjVr1KlTJ8XGxurIkSP65ptvtGjRIk2ZMiXX5wYHB6t+/foaPny4MjMzVbp0ac2fP187duxw63f8+HGVKVNG7dq1U/Xq1RUYGKiFCxdq5cqVrtmgxYsX66mnntJ9992nihUr6syZM5o8ebK8vb31j3/8Ix+vhIsLDg7Wu+++qwcffFA1a9ZUx44dFR4ert27d+vLL79UQkKCxowZc8X7wWXw5CUgKHr++OMP8+ijj5q4uDjj6+trgoKCTEJCghk9erTbTaIyMzPNoEGDTHx8vPHx8TFly5a96I2kLnTh5YN5XdJlzNkbRN14443G19fXVKpUyXz88cc5LuNctGiRad26tYmJiTG+vr4mJibGdOrUyfzxxx859nHhpY4LFy40CQkJxs/PzwQHB5tWrVrleSOpCy8TzeumNxc6/zLOvOR1GWefPn1MdHS08fPzMwkJCWb58uW5Xn45e/ZsU6VKFVOsWLFcbySVm/PHSU1NNbGxsaZmzZomMzPTrV+vXr2Ml5dXrjdSuhJnzpwx77//vrnjjjtMSEiI8fHxMbGxsaZLly5ul3gePXrUdOnSxZQqVcoEBgaapk2bmk2bNuW4NNAYY9577z1Trlw54+3tneNGUjrv8k2Hw2HCwsLMvffe63ZTpHNWr15tmjZtagIDA42/v79p1KiRWbZsWY5+524kFRoaaooXL27q1KmT40ZS48aNM/Xr1zclS5Y0TqfTlC9f3vTt29ekpKS49Xv55ZdN6dKljZeXV74v6Tz32o+IiDDFihUz4eHhplWrVmb27NmuPrm99vfu3Wvatm1rQkNDTUhIiLnvvvvMvn37jCQzYMAAY8zZW5337dvXVK9e3XVDrerVq5t33nnHNc727dvNww8/bMqXL++6yVejRo3MwoUL3eq83Ms4z1myZIlp2rSpCQkJMcWLFzfly5c3nTt3Nr/88ourT37eZyg4DmMsVn8BAACINRAAAOAyECAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAa/+Td6J8aMr6S3cC4DFvtqnq6RIA5KGEf+6/NPBCzEAAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCNAAEAAKwRIAAAgDUCBAAAsEaAAAAA1ggQAADAGgECAABYI0AAAABrBAgAAGCtmKcLACSpbbVIta0W6da2LyVd/b78Q5IUUryYOt4crapRgfLz8db+1AzN2XBAv+xJ9US5ACQdPHhAb785Ust//F4Z6ekqU/Y6/XvgEFWueqOnS8NVQIBAkbH3WLpeXbzd9XOWMa4/P3ZbWfn7euuNpTt1PD1Lt8WF6qmEWA2Yt0W7jqZ7olzgmpaamqLHOv9TtWrX0agx41SiRJj27N6loOBgT5eGq4QAgSIjyxilpJ/Jddv1pfw1ceWf2n7klCRpzoaDanZDKcWF+RMgAA+Y/OEERUZF6cVBQ11tMaXLeLAiXG0eDRCHDx/WBx98oOXLlys5OVmSFBUVpdtvv12dO3dWeHi4J8vDVRYV5NSbbSorMztbWw+nafraZB1Jy5QkbTmcpltjQ7Vu33Glnc5SndgQ+Xh7aeOBEx6uGrg2ff/dYt16ez290PcZrVn1i8IjIpTYvpPaJN7n6dJwlTiMOW+e+CpauXKlmjZtKn9/fzVu3FiRkWe//z5w4IAWLVqktLQ0zZs3T7fccstFx8nIyFBGRoZb2xNf/CFvH99Cqx0F76boIDmLeSn5eIZC/YqpzY2RKuHvoxe+/EPpZ7Ll7+OlJ+vFqlp0kM5kG50+k60xP+zSb8kEiL+jN9tU9XQJuEL169aQJHV6IEl3NmmqjRt+06gRw/TcCwPU8t42Hq0NV6aEv3e++nksQNx6662qXr26xo4dK4fD4bbNGKNu3bpp/fr1Wr58+UXHGThwoAYNGuTWdlNiN1X/xxMFXjOuHn8fL73eurKmrN6npduP6sFaMSpX0l/T1yXreMYZ1SoTrKY3hGvIgm3am8JXGH83BIi/v3q1b1LlKjfqvUlTXG0jXx2ijRt+0/sfTfVgZbhS+Q0QHruMc926derVq1eO8CBJDodDvXr10tq1ay85Tv/+/ZWSkuL2uPHeRwqhYlxNaZnZSj6eocggpyICfdWkUim9//Me/X7ghPYcS9es3w5q519palyxpKdLBa5JpUqFK65cebe2uPjyOpC830MV4WrzWICIiorSihUr8ty+YsUK19caF+N0OhUcHOz24OuLvz9nMS9FBPrq2KlM+XqffZleOFeWbaRc8ieAq+CmGjW1e9cOt7Y9u3cqKjrGQxXhavPYIspnn31Wjz32mFatWqW77rorxxqI9957T6+99pqnysNV1vHmaK35M1VHTp5WqJ+PEqtFKttIP+06prTTWUo+nqHOdUrr0zX7dSIjSzXLBKtqVKBe/26np0sHrkkdH3hIj3b+pyZOGKe7mjTT7xt+1ayZ09XvxYGeLg1XicfWQEjStGnTNGrUKK1atUpZWVmSJG9vb9WqVUu9e/dW+/btL2vch6asL8gycRV0T7hOlcIDFOj01vGMM/rjUJpmrEvWwROnJUmRQb5qXz1aFcP9VdzHWweOZ+irjYe0bOcxzxaOy8IaiP8NPyz9Vu+OHqU9u3cpunQZdXogiasw/gcU+UWU58vMzNThw4clSaVKlZKPj88VjUeAAIo2AgRQdOU3QBSJG0n5+PgoOjra02UAAIB84pdpAQAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwFqx/HSaM2dOvge89957L7sYAADw95CvANGmTZt8DeZwOJSVlXUl9QAAgL+BfAWI7Ozswq4DAAD8jbAGAgAAWMvXDMSFTp48qe+++067d+/W6dOn3bb16NGjQAoDAABFl3WAWLNmjVq0aKG0tDSdPHlSYWFhOnz4sPz9/RUREUGAAADgGmD9FUavXr3UqlUrHT16VH5+fvrpp5+0a9cu1apVS6+99lph1AgAAIoY6wCxdu1a9enTR15eXvL29lZGRobKli2r4cOH64UXXiiMGgEAQBFjHSB8fHzk5XX2aREREdq9e7ckKSQkRHv27CnY6gAAQJFkvQbi5ptv1sqVK3X99derQYMGeumll3T48GFNnjxZN954Y2HUCAAAihjrGYihQ4cqOjpakjRkyBCVKFFCTzzxhA4dOqTx48cXeIEAAKDosZ6BuOWWW1x/joiI0DfffFOgBQEAgKKPG0kBAABr1jMQ8fHxcjgceW7fvn37FRUEAACKPusA8cwzz7j9nJmZqTVr1uibb75R3759C6ouAABQhFkHiJ49e+ba/vbbb+uXX3654oIAAEDRV2BrIJo3b66ZM2cW1HAAAKAIK7AAMWPGDIWFhRXUcAAAoAi7rBtJnb+I0hij5ORkHTp0SO+8806BFne5xre/ydMlALiIErWf8nQJAPJwas2YfPWzDhCtW7d2CxBeXl4KDw9Xw4YNdcMNN9gOBwAA/oYcxhjj6SIKWvoZT1cA4GKYgQCKrvzOQFivgfD29tbBgwdztB85ckTe3t62wwEAgL8h6wCR14RFRkaGfH19r7ggAABQ9OV7DcRbb70lSXI4HHr//fcVGBjo2paVlaWlS5eyBgIAgGtEvgPEqFGjJJ2dgRg7dqzb1xW+vr6Ki4vT2LFjC75CAABQ5OQ7QOzYsUOS1KhRI33++ecqUaJEoRUFAACKNuvLOJcsWVIYdQAAgL8R60WU//jHP/Tqq6/maB8+fLjuu+++AikKAAAUbdYBYunSpWrRokWO9ubNm2vp0qUFUhQAACjarAPEiRMncr1c08fHR6mpqQVSFAAAKNqsA0S1atU0bdq0HO2ffvqpqlSpUiBFAQCAos16EeWLL76oxMREbdu2TXfeeackadGiRZoyZYpmzJhR4AUCAICixzpAtGrVSrNmzdLQoUM1Y8YM+fn5qXr16lq8eDG/zhsAgGvEFf8yrdTUVE2dOlUTJkzQqlWrlJWVVVC1XTZ+mRZQtPHLtICiq9B+mdY5S5cuVVJSkmJiYjRy5Ejdeeed+umnny53OAAA8Ddi9RVGcnKyJk6cqAkTJig1NVXt27dXRkaGZs2axQJKAACuIfmegWjVqpUqVaqk9evX64033tC+ffs0evTowqwNAAAUUfmegfj666/Vo0cPPfHEE7r++usLsyYAAFDE5XsG4ocfftDx48dVq1Yt1a1bV2PGjNHhw4cLszYAAFBE5TtA3HrrrXrvvfe0f/9+Pf744/r0008VExOj7OxsLViwQMePHy/MOgEAQBFyRZdxbt68WRMmTNDkyZN17NgxNWnSRHPmzCnI+i4Ll3ECRRuXcQJFV6FfxilJlSpV0vDhw7V3715NnTr1SoYCAAB/I1d8I6miiBkIoGhjBgIouq7KDAQAALg2ESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgSKrFW/rNTT3bupccN6ql61khYvWujpkoBrQkLN8prxxuPaPn+ITq0Zo1YNb8rR58UnWmr7/CH6a/nr+nLsUyp/XXiOPs3qVdXSj57VX8tf177vhuuz1x+9GuXjKiFAoMg6dSpNlSpVUv9/D/B0KcA1JcDPqV//+FPPDJuW6/Y+nRure6cG6jH0U9V/6DWdPHVa/337STl9i7n6tLmrhia88pA+mvOT6nT4j+7s8rqmff3L1ToEXAXFLt0F8Ix6dzRQvTsaeLoM4Joz/8ffNf/H3/Pc/uT9jfTqe/M099tfJUldX/xIuxYO072Nqmv6vFXy9vbSa33/oRfemKVJs5a7nrdpe3Kh146rhxkIAEC+xZUuqejwEC3+eZOrLfVEulb+tlN1b4qTJN18Q1mVjiyh7Gyj5VOf1/b5QzRrzBOqUj7aQ1WjMPztA0RGRoZSU1PdHhkZGZ4uCwD+J0WVCpYkHfzruFv7wSPHFVny7Lb4MqUkSf/u1kKvvj9P/+g5VsdST2neez1VItj/6haMQlOkA8SePXv08MMPX7TPsGHDFBIS4vYY8eqwq1QhAOBCXg6HJOnV9+dp1qK1WrNxjx4b8LGMjBKb3Ozh6lBQinSA+OuvvzRp0qSL9unfv79SUlLcHn2f73+VKgSAa0vy4VRJUkRYkFt7RMkgHThydtv+wymSpE3b97u2n848o517j6hsVNhVqhSFzaOLKOfMmXPR7du3b7/kGE6nU06n060t/cwVlQUAyMPOP49o/6EUNapbSev/+FOSFBRQXLVvjNN703+QJK3ZuEfpGZm6Pi5Sy9ae/f94sWJeui4mTLv3/+Wx2lGwPBog2rRpI4fDIWNMnn0c/zcVhmtP2smT2r17t+vnP/fu1aaNGxUSEqLomBgPVgb8bwvw81X5sv//vg5xpUvqpoqldTQ1TXuSj+rtKUv0fNdm2rr7kHb+eUQDurfU/kMpmrNknSTp+Ml0vT/jB73YrYX2Jh/V7v1/qVdSY0nS5wtWe+SYUPAc5mL/ehey0qVL65133lHr1q1z3b527VrVqlVLWVlZVuMyA/G/YeWKn9W1y0M52u9t3VYvD/2PBypCQSlR+ylPl4CLuKPW9Zr/fs8c7ZPn/KTHBnws6eyNpB5OTFBokJ+Wrd2mnkM/09bdB119ixXz0stPt1anlrXl5/TRyt92qe+IGdrIpZxF3qk1Y/LVz6MB4t5771WNGjU0ePDgXLevW7dON998s7Kzs63GJUAARRsBAii68hsgPPoVRt++fXXy5Mk8t1eoUEFLliy5ihUBAID88OgMRGFhBgIo2piBAIqu/M5AFOnLOAEAQNFEgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALBGgAAAANYIEAAAwBoBAgAAWCNAAAAAawQIAABgjQABAACsESAAAIA1AgQAALDmMMYYTxcBXExGRoaGDRum/v37y+l0erocAOfh/XntIkCgyEtNTVVISIhSUlIUHBzs6XIAnIf357WLrzAAAIA1AgQAALBGgAAAANYIECjynE6nBgwYwAItoAji/XntYhElAACwxgwEAACwRoAAAADWCBAAAMAaAQIAAFgjQKBIe/vttxUXF6fixYurbt26WrFihadLAiBp6dKlatWqlWJiYuRwODRr1ixPl4SrjACBImvatGnq3bu3BgwYoNWrV6t69epq2rSpDh486OnSgGveyZMnVb16db399tueLgUewmWcKLLq1q2r2rVra8yYMZKk7OxslS1bVk8//bT69evn4eoAnONwOPTFF1+oTZs2ni4FVxEzECiSTp8+rVWrVqlx48auNi8vLzVu3FjLly/3YGUAAIkAgSLq8OHDysrKUmRkpFt7ZGSkkpOTPVQVAOAcAgQAALBGgECRVKpUKXl7e+vAgQNu7QcOHFBUVJSHqgIAnEOAQJHk6+urWrVqadGiRa627OxsLVq0SLfddpsHKwMASFIxTxcA5KV3795KSkrSLbfcojp16uiNN97QyZMn1aVLF0+XBlzzTpw4oa1bt7p+3rFjh9auXauwsDBdd911HqwMVwuXcaJIGzNmjEaMGKHk5GTVqFFDb731lurWrevpsoBr3rfffqtGjRrlaE9KStLEiROvfkG46ggQAADAGmsgAACANQIEAACwRoAAAADWCBAAAMAaAQIAAFgjQAAAAGsECAAAYI0AAQAArBEgABSazp07q02bNq6fGzZsqGeeeeaq1/Htt9/K4XDo2LFjV33fwP8qAgRwDercubMcDoccDod8fX1VoUIFDR48WGfOnCnU/X7++ed6+eWX89WXf/SBoo1fpgVco5o1a6YPP/xQGRkZ+uqrr/Tkk0/Kx8dH/fv3d+t3+vRp+fr6Fsg+w8LCCmQcAJ7HDARwjXI6nYqKilJsbKyeeOIJNW7cWHPmzHF97TBkyBDFxMSoUqVKkqQ9e/aoffv2Cg0NVVhYmFq3bq2dO3e6xsvKylLv3r0VGhqqkiVL6rnnntOFv2rnwq8wMjIy9Pzzz6ts2bJyOp2qUKGCJkyYoJ07d7p+UVOJEiXkcDjUuXNnSWd/rfuwYcMUHx8vPz8/Va9eXTNmzHDbz1dffaWKFSvKz89PjRo1cqsTQMEgQACQJPn5+en06dOSpEWLFmnz5s1asGCB5s6dq8zMTDVt2lRBQUH6/vvv9eOPPyowMFDNmjVzPWfkyJGaOHGiPvjgA/3www/666+/9MUXX1x0nw899JCmTp2qt956Sxs3btS4ceMUGBiosmXLaubMmZKkzZs3a//+/XrzzTclScOGDdNHH32ksWPHasOGDerVq5ceeOABfffdd5LOBp3ExES1atVKa9euVdeuXdWvX7/COm3AtcsAuOYkJSWZ1q1bG2OMyc7ONgsWLDBOp9M8++yzJikpyURGRpqMjAxX/8mTJ5tKlSqZ7OxsV1tGRobx8/Mz8+bNM8YYEx0dbYYPH+7anpmZacqUKePajzHGNGjQwPTs2dMYY8zmzZuNJLNgwYJca1yyZImRZI4ePepqS09PN/7+/mbZsmVufR955BHTqVMnY4wx/fv3N1WqVHHb/vzzz+cYC8CVYQ0EcI2aO3euAgMDlZmZqezsbN1///0aOHCgnnzySVWrVs1t3cO6deu0detWBQUFuY2Rnp6ubdu2KSUlRfv371fdunVd24oVK6Zbbrklx9cY56xdu1be3t5q0KBBvmveunWr0tLS1KRJE7f206dP6+abb5Ykbdy40a0OSbrtttvyvQ8A+UOAAK5RjRo10rvvvitfX1/FxMSoWLH//7+DgIAAt74nTpxQrVq19Mknn+QYJzw8/LL27+fnZ/2cEydOSJK+/PJLlS5d2m2b0+m8rDoAXB4CBHCNCggIUIUKFfLVt2bNmpo2bZoiIiIUHByca5/o6Gj9/PPPql+/viTpzJkzWrVqlWrWrJlr/2rVqik7O1vfffedGjdunGP7uRmQrKwsV1uVKlXkdDq1e/fuPGcuKleurDlz5ri1/fTTT5c+SABWWEQJ4JL++c9/qlSpUmrdurW+//577dixQ99++6169OihvXv3SpJ69uyp//znP5o1a5Y2bdqk7t27X/QeDnFxcUpKStLDDz+sWbNmucb87LPPJEmxsbFyOByaO3euDh06pBMnTigoKEjPPvusevXqpUmTJmnbtm1avXq1Ro8erUmTJkmSunXrpi1btqhv377avHmzpkyZookTJxb2KQKuOQQIAJfk7++vpUuX6rrrrlNiYqIqV66sRx55ROnp6a4ZiT59+ujBBx9UUlKSbrvtNgUFBalt27YXHffdd99Vu3bt1L17d91www169NFHdfLkSUlS6dKlNWjQIPXr10+RkZF66qmnJEkvv/yyXnzxRQ0bNkyVK1dWs2bN9OWXXyo+Pl6SdN1112nmzJmaNWuWqlevrrFjx2ro0KGFeHaAa5PD5LXCCQAAIA/MQAAAAGsECAAAYI0AAQAArBEgAACANQIEAACwRoAAAADWCBAAAMAaAQIAAFgjQAAAAGsECAAAYI0AAQAArP0/p47X6M2WQPQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Answer:\n",
        "1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Understand the dataset\n",
        "\n",
        "Identify numeric vs categorical features.\n",
        "\n",
        "Check missing values and class imbalance.\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Numeric features: impute using median (robust to outliers).\n",
        "\n",
        "Categorical features: impute using mode or a special category like \"Unknown\".\n",
        "\n",
        "Encode categorical features\n",
        "\n",
        "For AdaBoost/XGBoost:\n",
        "\n",
        "Use One-Hot Encoding for low-cardinality categories.\n",
        "\n",
        "Use Target/Ordinal Encoding for high-cardinality categories.\n",
        "\n",
        "For CatBoost:\n",
        "\n",
        "Pass categorical columns directly → CatBoost handles them natively with ordered target encoding.\n",
        "\n",
        "Feature scaling\n",
        "\n",
        "Boosting trees usually don’t require scaling, so standardization is optional.\n",
        "\n",
        "Handle class imbalance\n",
        "\n",
        "Use techniques like:\n",
        "\n",
        "SMOTE / ADASYN to oversample minority class.\n",
        "\n",
        "Class weights in the boosting model (most boosting frameworks support this).\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "Algorithm\tPros for this scenario\n",
        "AdaBoost\tSimple, interpretable, works with small datasets, but may struggle with missing values and high-cardinality categorical features.\n",
        "XGBoost\tFast, robust, handles numeric and encoded categorical features, supports regularization to prevent overfitting.\n",
        "CatBoost\tHandles categorical features natively, robust to missing values, less hyperparameter tuning required, often best for datasets with mixed feature types and imbalances.\n",
        "\n",
        "Step: For this dataset, CatBoost is the most efficient because it natively handles both categorical features and missing values, and works well with imbalanced classes.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Select key hyperparameters\n",
        "\n",
        "Number of trees (iterations / n_estimators)\n",
        "\n",
        "Tree depth (depth / max_depth)\n",
        "\n",
        "Learning rate (learning_rate)\n",
        "\n",
        "Regularization parameters (l2_leaf_reg for CatBoost, lambda/alpha for XGBoost)\n",
        "\n",
        "Subsampling (subsample, colsample_bytree)\n",
        "\n",
        "Tuning approach\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with stratified k-fold cross-validation.\n",
        "\n",
        "Focus on metrics suitable for imbalanced datasets (see below).\n",
        "\n",
        "Early stopping\n",
        "\n",
        "Monitor validation set performance to stop training before overfitting.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Since the dataset is imbalanced, accuracy alone is insufficient. Focus on:\n",
        "\n",
        "ROC-AUC → measures overall model discrimination between default vs non-default.\n",
        "\n",
        "Precision & Recall → capture performance on the minority class (defaults).\n",
        "\n",
        "F1-score → balances precision and recall for actionable insights.\n",
        "\n",
        "Confusion Matrix → helps visualize true positives, false positives, etc.\n",
        "\n",
        "In credit risk, false negatives (predicting no default when default occurs) are costlier than false positives, so recall for the default class is particularly important.\n",
        "\n",
        "5. How the Business Benefits\n",
        "\n",
        "Reduce financial risk → identify high-risk borrowers before issuing loans.\n",
        "\n",
        "Better customer targeting → allocate risk-based interest rates or require additional collateral.\n",
        "\n",
        "Regulatory compliance → boosting models (especially tree-based) allow feature importance analysis, improving transparency for audits.\n",
        "\n",
        "Optimized operations → resources (loan approval checks, fraud investigations) can be focused on high-risk cases.\n",
        "\n",
        "6. Example Workflow (Pseudo-Code)"
      ],
      "metadata": {
        "id": "ZmAJPgLzFm57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Example: assume df is your loan dataset\n",
        "# df = pd.read_csv(\"loan_data.csv\")\n",
        "# X = df.drop(\"default\", axis=1)\n",
        "# y = df[\"default\"]\n",
        "\n",
        "# For illustration, let's create dummy data\n",
        "import numpy as np\n",
        "X = pd.DataFrame(np.random.rand(1000, 10), columns=[f\"feature_{i}\" for i in range(10)])\n",
        "y = np.random.randint(0, 2, 1000)\n",
        "categorical_columns = []  # list indices of categorical columns if any\n",
        "\n",
        "# 1. Handle missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.05,\n",
        "    depth=4,\n",
        "    eval_metric='AUC',\n",
        "    cat_features=categorical_columns,\n",
        "    verbose=0,\n",
        "    random_seed=42\n",
        ")\n",
        "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
        "\n",
        "# 4. Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 5. Print metrics\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mfMUBk7E4P0",
        "outputId": "d5c8cf04-7d39-4287-ce7a-d046d658ba7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.521\n",
            "Precision: 0.495, Recall: 0.331, F1-score: 0.397\n",
            "Confusion Matrix:\n",
            " [[110  48]\n",
            " [ 95  47]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJxkeEAAF2yJ"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}